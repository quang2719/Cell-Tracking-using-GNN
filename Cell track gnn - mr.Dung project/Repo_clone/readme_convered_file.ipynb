{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\">\n",
       "\n",
       "# Graph Neural Network for Cell Tracking in Microscopy Videos ([ECCV 2022](https://www.springerprofessional.de/graph-neural-network-for-cell-tracking-in-microscopy-videos/23632850))\n",
       "\n",
       "\n",
       "[![Arxiv](https://img.shields.io/badge/ArXiv-2202.04731-orange.svg?color=red)](https://arxiv.org/abs/2202.04731)\n",
       "\n",
       "  Official Implementation: **Graph Neural Network for Cell Tracking in Microscopy Videos** \n",
       "\n",
       " [**Project Page**](https://talbenha.github.io/cell-tracker-gnn/)\n",
       "\n",
       "  <img width=\"1549\" alt=\"model\" src=\"https://user-images.githubusercontent.com/57532696/177995954-f8d4c6e8-338a-4d2b-a244-e5c9e912dbe9.png\">\n",
       "\n",
       "![bmp2_control_concat1](https://user-images.githubusercontent.com/57532696/178099404-3af32932-e723-4ec6-a0fc-a30d3f38f67a.gif)\n",
       "\n",
       "\n",
       "\n",
       "</div>\n",
       "\n",
       "<p></div>\n",
       "<br><br></p>\n",
       "<h2>Preliminaries</h2>\n",
       "<details>\n",
       "<summary><b> Our implementation integrates <a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n",
       "<a href=\"https://pytorchlightning.ai/\"><img alt=\"Lightning\" src=\"https://img.shields.io/badge/-Lightning-792ee5\"></a>\n",
       "<a href=\"https://pytorch-geometric.readthedocs.io/en/latest/\"><img alt=\"PyG\" src=\"https://img.shields.io/badge/-PyG-blue\"></a>\n",
       "<a href=\"https://hydra.cc/\"><img alt=\"Config: Hydra\" src=\"https://img.shields.io/badge/Config-Hydra-89b8cd\"></a> libraries:  </b></summary>\n",
       "\n",
       "[PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) is a lightweight PyTorch wrapper for high-performance AI research.\n",
       "\n",
       "[PyG (PyTorch Geometric)](https://pytorch-geometric.readthedocs.io/en/latest/) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n",
       "\n",
       "[Hydra](https://github.com/facebookresearch/hydra) is an open-source Python framework that simplifies the development of research and other complex applications.\n",
       "\n",
       "> If you are not familiar with [PyTorch](https://pytorch.org), [PyTorch Lightning](https://www.pytorchlightning.ai), [PyG](https://pytorch-geometric.readthedocs.io/en/latest/) and [Hydra](https://hydra.cc). We highly recommend to read about them before starting.\n",
       ">\n",
       "We use older version of the publicly available deep learning template provided in <a href=\"https://github.com/hobogalaxy/lightning-hydra-template\"><img alt=\"Template\" src=\"https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray\"></a>\n",
       "\n",
       "</details>\n",
       "\n",
       "<h2>Set up conda virtual environment</h2>\n",
       "<details>\n",
       "<summary><b>Install dependencies on linux enviroment (click to expand):</b></summary>\n",
       "we provide conda envrioment setup dependencies - if you are not familiar with conda, please read about before starting\n",
       "\n",
       "```yaml\n",
       "# Enter to the code folder\n",
       "cd cell-tracker-gnn\n",
       "\n",
       "# create conda environment python=3.8 pytorch==1.8.0 torchvision==0.9.0 cudatoolkit=11.1 faiss-gpu pytorch-lightning==1.4.9\n",
       "conda create --name cell-tracking-challenge --file requirements-conda.txt\n",
       "conda activate cell-tracking-challenge\n",
       "\n",
       "# install other requirements\n",
       "pip install -r requirements.txt\n",
       "```\n",
       "</details>\n",
       "\n",
       "<h2>Structure</h2>\n",
       "<details>\n",
       "<summary><b>The directory structure of our implementation looks like (click to expand):</b></summary>\n",
       "\n",
       "```\n",
       "├── configs                 <- Hydra configuration files\n",
       "│   ├── callbacks               <- Callbacks configs\n",
       "│   ├── datamodule              <- Datamodule configs\n",
       "│   ├── feat_extract            <- Feature extraction configs\n",
       "│   ├── logger                  <- Logger configs\n",
       "│   ├── metric_learning         <- Metric learning configs\n",
       "│   ├── model                   <- Model configs\n",
       "│   ├── trainer                 <- Trainer configs\n",
       "│   │\n",
       "│   ├── config.yaml             <- Main project configuration file\n",
       "│\n",
       "├── data                    <- Project data\n",
       "│\n",
       "├── logs                    <- Logs generated by Hydra and PyTorch Lightning loggers\n",
       "│\n",
       "├── outputs                 <- Outputs generated by Hydra and tensorboard loggers when training deep metric learning model\n",
       "│\n",
       "│\n",
       "├── src\n",
       "│   ├── callbacks                   <- Lightning callbacks\n",
       "│   ├── datamodules                 <- Lightning datamodules and dataset files used\n",
       "│   │   ├── datasets                             <- Graph Dataset implementation\n",
       "│   │   │   └── graph_dataset.py                          <- Graph Dataset implementation\n",
       "│   │   ├── extract_features                     <- Extract  features used for graph\n",
       "│   │   │   ├── preprocess_seq2graph_2d.py                <- Extract  features for 2d dataset with full segmentation\n",
       "│   │   │   ├── preprocess_seq2graph_3D.py                <- Extract  features for 3d dataset\n",
       "│   │   │   └── preprocess_seq2graph_patch_based.py       <- Extract  features for 2d dataset with markers annotations\n",
       "│   │   ├── celltrack_datamodule.py              <- Lightning datamodules implementing split for train, valid and test using separate sequences for each\n",
       "│   │   └── celltrack_datamodule_mulSeq.py       <- Lightning datamodules implementing split for train, valid and test using combine sequences for each\n",
       "│   │\n",
       "│   ├── metrics                     <- Lightning metrics use to track performances\n",
       "│   ├── models                      <- Lightning model + PyTorch models +  PyTorch Geometric model\n",
       "│   │   ├── modules                             <- models implementation\n",
       "│   │   │   ├── celltrack_model.py                        <- complete model implementation\n",
       "│   │   │   ├── edge_mpnn.py                              <- Edge-oriented message passing implementation\n",
       "│   │   │   ├── mlp.py                                    <- multilayer perceptron implementation\n",
       "│   │   │   └── pdn_conv.py                               <- PDN-Conv implementation\n",
       "│   │   └── celltrack_plmodel.py                <- Lightning model implementing training routine\n",
       "│   ├── utils                   <- Utility scripts\n",
       "│   │   └── utils.py                            <- Utils features\n",
       "│   │\n",
       "│   └── train.py                                <- Training pipeline\n",
       "│\n",
       "├── src_metric_learning\n",
       "│   ├── Data               <- Data modules - datasets and sampler\n",
       "│   │   ├── dataset_2D.py       <- Implemetation of 2D dataset\n",
       "│   │   ├── dataset_3D.py       <- Implemetation of 3D dataset\n",
       "│   │   └── sampler.py          <- Implemetation of sampler used for batch construction\n",
       "│   ├── modules                 <- Pytorch models\n",
       "│   │   ├── resnet_2d           <- Implemetation of ResNet for 2D dataset\n",
       "│   │   │   ├── resnet.py             <- Final models\n",
       "│   │   │   └── utils_resnet.py       <- Multiple ResNet blocks and models Implemetation\n",
       "│   │   ├── resnet_3d           <- Implemetation of ResNet for 3D dataset\n",
       "│   │   │   ├── resnet.py             <- Final models\n",
       "│   └── └── └── utils_resnet.py       <- Multiple ResNet blocks and models Implemetation\n",
       "│\n",
       "├── LICENSE                 <- Attribution-NonCommercial 4.0 International\n",
       "├── README.md               <- All information\n",
       "│\n",
       "├── requirements.txt              <- File for installing python dependencies (specification of dependencies)\n",
       "├── requirements-conda.txt        <- File for conda environment creation (specification of dependencies)\n",
       "│\n",
       "├── run.py                             <- Run training of the complete model with any pipeline configuration of 'configs/config.yaml'\n",
       "├── run_feat_extract.py                <- Run feature extraction pipeline 'configs/feat_extract/feat_extract.yaml' configuration file\n",
       "└── run_train_metric_learning.py       <- Run training of any settings using 'configs/metric_learning/...' configuration files\n",
       "```\n",
       "</details>\n",
       "\n",
       "<h2>Data</h2>\n",
       "<details>\n",
       "\n",
       "### Data Structure\n",
       "Recommended Data directory should look like (click to expand):\n",
       "```\n",
       "├── data                    <- Project data\n",
       "│   ├── CTC                 <- Cell tracking challenge data\n",
       "│   │   ├── Training                             <- Training Split\n",
       "│   │   │   ├── Fluo-N2DH-SIM+                        <- Fluo-N2DH-SIM+ Dataset\n",
       "│   │   │   │   ├── 01                                    <- Seuqence 01\n",
       "│   │   │   │   ├── 01_GT                                 <- Seuqence 01 GT\n",
       "│   │   │   │   │   ├── TRA                                   <- Tracking GT\n",
       "│   │   │   │   │   └── SEG                                   <- Tracking SEG (Not used)\n",
       "│   │   │   │   ├── 02                                    <- Seuqence 02\n",
       "│   │   │   │   ├── 02_GT                                 <- Seuqence 02 GT\n",
       "│   │   │   │   │   ├── TRA                                   <- Tracking GT\n",
       "│   │   │   │   │   └── SEG                                   <- Tracking SEG (Not used)\n",
       "│   │   │   ├── PhC-C2DH-U373                             <- PhC-C2DH-U373 Dataset\n",
       "│   │   │   │   ├── 01                                    <- Seuqence 01\n",
       "│   │   │   │   ├── 01_GT                                 <- Seuqence 01 GT\n",
       "│   │   │   │   │   ├── TRA                                   <- Tracking GT\n",
       "│   │   │   │   │   └── SEG                                   <- Tracking SEG (Not used)\n",
       "│   │   │   │   ├── 01_ST                                 <- Seuqence 01 Silver GT\n",
       "│   │   │   │   └── └── SEG                                   <- Tracking SEG\n",
       "│   │   │   .\n",
       "│   │   │   .\n",
       "│   │   │   .\n",
       "│   │   ├── Test                             <- Graph Dataset implementation\n",
       "│   │   │   ├── Fluo-N2DH-SIM+                        <- Fluo-N2DH-SIM+ Dataset\n",
       "│   │   │   │   ├── 01                                    <- Seuqence 01\n",
       "│   │   │   │   ├── 02                                    <- Seuqence 02\n",
       "│   │   │   ├── PhC-C2DH-U373                             <- PhC-C2DH-U373 Dataset\n",
       "│   │   │   │   ├── 01                                    <- Seuqence 01\n",
       "│   │   │   │   ├── 02                                    <- Seuqence 02\n",
       "│   │   │   .\n",
       "│   │   │   .\n",
       "│   │   │   .\n",
       "```\n",
       "\n",
       "### Download Datasets\n",
       "- **[Cell tracking challenge 2D datasets](http://celltrackingchallenge.net/2d-datasets/)**: \n",
       "- **[Cell tracking challenge 3D datasets](http://celltrackingchallenge.net/3d-datasets/)**: \n",
       "</details>\n",
       "\n",
       "<h2>Training code</h2>\n",
       "<h3>Overview</h3>\n",
       "<details>\n",
       "\n",
       "Our code consists of 3 run files located on the 'home' directory of the project -```run.py```, ```run_feat_extract.py```, and ```run_train_metric_learning.py```- dividing our project into 3 parts namely 'complete model', 'feature extraction', and 'metric learning', respectively. An overview of each is provided in the next few sentences:\n",
       "- **Metric Learning**: is responsible for training a model for extracting features using the Pytorch Metric Learning library and building using a separate source code.(see src_metric_learning in  [#Project Structure](#project-structure)). Before running this part, we should generate CSV files consisting of relevant information about the cells, used by the datasets in metric learning training.\n",
       "- **Feature Extraction**: After training a discriminative model to extract features, we are extracting features used later to build our graphs.\n",
       "- **Complete Model**: When all the required data is ready, we can use it to train a graph neural network model as presented in the main paper.\n",
       "<br>\n",
       "</details>\n",
       "\n",
       "<h3>Command lines Summary</h3>\n",
       "<details>\n",
       "\n",
       "We summarize all the relevant command lines to produce a run, an explanation for each variable is provided in **Training code** Section below.\n",
       "```yaml\n",
       " export CUDA_VISIBLE_DEVICES=0 # select GPU number\n",
       "\n",
       "# run feat_extract for metric learning -\n",
       "# please ensure that your target is correct in the 'configs/feat_extract/feat_extract.yaml' file.\n",
       "python run_feat_extract.py params.input_images=<image_dir> params.input_masks=<masks_dir> params.input_seg=<masks_dir> params.output_csv=<save_output> params.basic=True params.sequences=[<str_sequences>, <str_sequences>, ...] params.seg_dir=<seg_dir_template>\n",
       "\n",
       "# run metric learning training -\n",
       "python run_train_metric_learning.py dataset.kwargs.data_dir_img=<image_directory> dataset.kwargs.data_dir_mask=<data_dir_mask> dataset.kwargs.dir_csv=<dir_csv>\n",
       "# output 'all_params.pth' is generated at end, it is the input_model for the next comand line\n",
       "\n",
       "# run feat_extract for cell tracking training -\n",
       "python run_feat_extract.py params.input_images=<image_dir> params.input_masks=<masks_dir> params.input_seg=<masks_dir> params.input_model=<path_to_all_params_produced_in_metric_learning> params.output_csv=<save_output> params.basic=False params.sequences=[<str_sequences>,<str_sequences>,...] params.seg_dir=<seg_dir_template>\n",
       "\n",
       "# cell tracking training run\n",
       "python run.py datamodule.dataset_params.main_path=<csv_home_directory> datamodule.dataset_params.exp_name=\"<name>_<2D/3D>\"\n",
       "```\n",
       "\n",
       "For example, if your data structure is organized as recommended, you can run training for Fluo-N2DH-SIM+ dataset with the following command lines:\n",
       "```yaml\n",
       " export CUDA_VISIBLE_DEVICES=0 # select GPU number\n",
       "\n",
       "# run feat_extract for metric learning -\n",
       "python run_feat_extract.py params.input_images=\"./data/CTC/Training/Fluo-N2DH-SIM+\" params.input_masks=\"./data/CTC/Training/Fluo-N2DH-SIM+\" params.input_seg=\"./data/CTC/Training/Fluo-N2DH-SIM+\" params.output_csv=\"./data/basic_features/\" params.sequences=['01','02']  params.seg_dir='_GT/TRA' params.basic=True\n",
       "\n",
       "# run metric learning training -\n",
       "python run_train_metric_learning.py dataset.kwargs.data_dir_img=\"./data/CTC/Training/Fluo-N2DH-SIM+\" dataset.kwargs.data_dir_mask=\"./data/CTC/Training/Fluo-N2DH-SIM+\" dataset.kwargs.dir_csv=\"./data/basic_features/Fluo-N2DH-SIM+\" dataset.kwargs.subdir_mask='GT/TRA'\n",
       "# output 'all_params.pth' is generated at end, it is the input_model for the next comand line\n",
       "\n",
       "# run feat_extract for cell tracking training -\n",
       "python run_feat_extract.py params.input_images=\"./data/CTC/Training/Fluo-N2DH-SIM+\" params.input_masks=\"./data/CTC/Training/Fluo-N2DH-SIM+\" params.input_seg=\"./data/CTC/Training/Fluo-N2DH-SIM+\" params.output_csv=\"./data/ct_features/\" params.sequences=['01','02']  params.seg_dir='_GT/TRA' params.basic=False params.input_model=<path_to_all_params_produced_in_metric_learning>\n",
       "\n",
       "# cell tracking training run\n",
       "python run.py datamodule.dataset_params.main_path=\"./data/ct_features/Fluo-N2DH-SIM+\" datamodule.dataset_params.exp_name=\"2D_SIM\" datamodule.dataset_params.drop_feat=[]\n",
       "```\n",
       "</details>\n",
       "\n",
       "<h3>Dive Into Details</h3>\n",
       "<details>\n",
       "We provide details on how to run any aspect of our code, from metric learning to our full model performing cell tracking, and extracting features in between.\n",
       "\n",
       "### Run Metric Learning\n",
       "1. Before running training, we should generate CSV files consisting of relevant information about the cells, we do so using ```run_feat_extract.py``` file and the corresponding configuration file located in ```configs/feat_extract/feat_extract.yaml```:\n",
       "```yaml\n",
       "defaults:\n",
       "    - params: params.yaml # do not change\n",
       "_target_: src.datamodules.extract_features.<choose_seq2graph_file> # options - preprocess_seq2graph_2d/preprocess_seq2graph_3D/preprocess_seq2graph_patch_based\n",
       "```\n",
       "Where the params ```configs/feat_extract/params/params.yaml``` configuration:\n",
       "```\n",
       "input_images: #Please/insert/path/to/image_frames\n",
       "input_masks: #Please/insert/path/to/image_masks/corresponds/image_frames\n",
       "input_seg: #Please/insert/path/to/segmentation_mask/corresponds/image_frames\n",
       "input_model: #Please/insert/path/of/metric_learning/feature_extractor_model\n",
       "output_csv: #Please/insert/path/to/save/features\n",
       "basic:  # !! Most important now- should be True !! -options True/False\n",
       "sequences: # example: ['01', '02']\n",
       "seg_dir: <choose_seg_dir_template> # options '_GT/SEG'/'_ST/SEG'\n",
       "```\n",
       "An explanation of each variable is detailed in the comments.\n",
       "\n",
       "In this stage, the 'basic' parameter is the most important one- should be set to True, indicating for basic features used for metric learning.\n",
       "The 'seg_dir' variable is used since in the cell tracking challenge (CTC) the partitions to folders are made in a fixed template. for example for sequence 1 - '01' folder is for images, '01_GT/TRA' folder is for markers annotation, '01_GT/SEG' folder is for segmentation annotation. In the case of silver ground truth segmentation, the folder is '01_ST/SEG'. We are following this assumption to all datasets, even datasets that are not in the CTC.\n",
       "\n",
       "After setting all paths, we can run ```run_feat_extract.py``` to extract features and the CSV files will be saved to the folder of 'output_csv' (Please pay attention to the log provided which indicates the place that the files saved).\n",
       "\n",
       "Now, you are familiar with all the relevant variables, we are providing a command line to produce the corresponding run with an override of the discussed variables.\n",
       "\n",
       "```yaml\n",
       "# run feat_extract for metric learning -\n",
       "# please ensure that your target is correct in the 'configs/feat_extract/feat_extract.yaml' file.\n",
       "python run_feat_extract.py params.input_images=<image_dir> params.input_masks=<masks_dir> params.input_seg=<masks_dir> params.output_csv=<save_output> params.basic=True params.sequences=[<str_sequences>, <str_sequences>, ...] params.seg_dir=<seg_dir_template>\n",
       "```\n",
       "\n",
       "\n",
       "2. After generating the required CSVs, the next step is to train a discriminative model to extract features using ```run_train_metric_learning.py``` and the corresponding configuration files in  ```configs/metric_learning```:\n",
       "- ```config_2D.yaml```: hyperparameters for training. Here we also set the 'exp_name' indicates for the folder name to save the outputs, and we can also choose between two optional settings for 2D datasets- those with a marker (```dataset/dataset_2D_patch_based.yaml```) and those with segmentations(```dataset/dataset_2D.yaml```).\n",
       "\n",
       "- ```config_3D.yaml```: hyperparameters for training. Here we also set the 'exp_name' indicates for the folder name to save the outputs, it works with the segmentation setting for 3D datasets(```dataset/dataset_3D.yaml```).\n",
       "- The default configuration is 2D datasets in ```run_train_metric_learning.py```. To change it, you should change the 'config_name' to ```config_3D.yaml``` in the following line included as part of ```run_train_metric_learning.py```: ```@hydra.main(config_path=\"configs/metric_learning/\", config_name=\"config_2D.yaml\")```.\n",
       "- ```dataset_**.yaml```- the important variables to set here are the paths to the images, masks, and CSV produced in *step 1* above.\n",
       "  ```yaml\n",
       "  data_dir_img: #Please/insert/path/to/images_directory\n",
       "  data_dir_mask: #Please/insert/path/to/segmentation_mask/corresponds/images\n",
       "  subdir_mask:  # options '_GT/SEG'/'_ST/SEG'/'GT/TRA'\n",
       "  dir_csv: #Please/insert/path/to/saved_basic_CSV\n",
       "  ```\n",
       "In case you work with marker, you should set 'subdir_mask' to 'GT/TRA', in case you have full segmentation by GT(set '_GT/SEG') or silver GT (set '_ST/SEG').\n",
       "\n",
       "A command line to produce the corresponding run with the override of the discussed variables is provided:\n",
       "  ```yaml\n",
       "python run_train_metric_learning.py dataset.kwargs.data_dir_img=<image_directory> dataset.kwargs.data_dir_mask=<data_dir_mask> dataset.kwargs.dir_csv=<dir_csv>\n",
       "    ```\n",
       "3. Now, you've set everything up and you're ready to run ```run_train_metric_learning.py```.\n",
       "At the end of the run, our code prepares wraps the best checkpoints and saves them with metadata in a dictionary file called \"/outputs/<date_time>/<time>/all_params.pth\" in the project directory. This dictionary is required for learned features extraction back in ```configs/feat_extract/params/params.yaml```:\n",
       "```\n",
       "input_images: #Please/insert/path/to/image_frames\n",
       "input_masks: #Please/insert/path/to/image_masks/corresponds/image_frames\n",
       "input_seg: #Please/insert/path/to/segmentation_mask/corresponds/image_frames\n",
       "input_model: #Please/insert/path/of/metric_learning/feature_extractor_model\n",
       "output_csv: #Please/insert/path/to/save/features\n",
       "basic:  # !! Most important now- should be True !! -options True/False\n",
       "sequences: # example: ['01', '02']\n",
       "seg_dir: <choose_seg_dir_template> # options '_GT/SEG'/'_ST/SEG'\n",
       "```\n",
       "\n",
       "The 'basic' variable should be set to 'False' and the input_model is the save dictionary ('all_params') file logged at the end of the training of the metric learning. You should now run again ```run_feat_extract.py``` to extract features - both spatio-temporal and deep metric learning features.\n",
       "\n",
       "\n",
       "A command line to produce the corresponding run with an override of the discussed variables is provided:\n",
       "```yaml\n",
       "# run feat_extract for cell tracking  -\n",
       "# please ensure that your target is correct in the 'configs/feat_extract/feat_extract.yaml' file.\n",
       "python run_feat_extract.py params.input_images=<image_dir> params.input_masks=<masks_dir> params.input_seg=<masks_dir> params.input_model=<path_to_all_params_produced_in_metric_learning> params.output_csv=<save_output> params.basic=False params.sequences=[<str_sequences>, <str_sequences>, ...] params.seg_dir=<seg_dir_template>\n",
       "```\n",
       "\n",
       "### Run Training of the full model Cell Tracking by GNN\n",
       "\n",
       "Our main file is ```run.py``` with the configuration  ```configs/config.yaml```. Main project config contains default training configuration:<br>\n",
       "  ```yaml\n",
       "defaults:\n",
       "    - trainer: default_trainer.yaml # do not change\n",
       "    - model: celltrack_model_patch_based.yaml # can be changed\n",
       "    - datamodule: datamodule_multiSequence.yaml # can be changed\n",
       "    - callbacks: default_callbacks.yaml  # do not change\n",
       "    - logger: many_loggers.yaml  # do not change\n",
       "```\n",
       "as mentioned in the comments, you can change the model and the datamodule configurations only, which are located in ```configs/model``` and  ```configs/datamodule``` folders, respectively.\n",
       "**model** is provided with 3 main option - ```celltrack_model_2d.yaml```, ```celltrack_model_3d.yaml```, and ```celltrack_model_patch_based.yaml``` indicate for 2D  dataset with segmentation, 3D  dataset with segmentation, and 2d  dataset with markers, respectively. The only differnece between each is the input features dimension.\n",
       "\n",
       "**datamodule** is provided with 2 main option -\n",
       "  1. Run with separted sequences for train/validation/test using ```datamodule_sepSequences.yaml```\n",
       "  2. Run with combination of sequences for train/validation/test using ```datamodule_multiSequence.yaml```- this configuration is used to train the final model to CTC (with 2 combination of the provided sequences to train and validation).<br>\n",
       "\n",
       "In each setting, you should change the directory in the variable \"main_path\" and 'dirs_path' sub-dirs of the main path. In case that you don't want to run with patch base settings (marker annotation settings) and you do want to run with segmentation annotation settings, please comment the strings in \"drop_feat\" argument, or simply override them with adding ```datamodule.dataset_params.drop_feat=[]``` to the run command line.\n",
       "\n",
       "**model** is provided with 3 main options -\n",
       "  1. Run with 2d+segmentation ```celltrack_model_2d.yaml```\n",
       "  2. Run with 2d+markers ```celltrack_model_patch_based.yaml```\n",
       "  3. Run with 3d+segmentation ```celltrack_model_3d.yaml```<br>\n",
       "\n",
       "In these configurations, no changes are requested. Just setting the preference settings in the main config file  ```configs/config.yaml```.\n",
       "\n",
       "Now, when you are familiar with all the relevant variables, we are providing a command line to produce the corresponding run with an override of the discussed variables.\n",
       "```yaml\n",
       "export CUDA_VISIBLE_DEVICES=0 # select GPU number\n",
       "# training run\n",
       "python run.py datamodule.dataset_params.main_path=<csv_home_directory> datamodule.dataset_params.exp_name=\"<name>_<2D/3D>\"\n",
       "```\n",
       "\n",
       "At the end of the training, a run is made to extract the validation set scores on the edges of the graph for the best checkpoint. Messages with the performance and the best checkpoint path are logged. The achieved precision, recall, and accuracy scores by our method on the edge classification are approximate ~99% (and even higher), and the scores are logged at this stage along with other information.\n",
       "\n",
       "\n",
       "Summary of all required command lines is provided in Section **Command lines Summary** above.\n",
       "</details>\n",
       "\n",
       "<h3>Logs Formats</h3>\n",
       "<details>\n",
       "Hydra creates a new working directory for every executed training run (metric_learning/cell_tracking). <br>\n",
       "By default, logs have the following structure separated for two main directories logs/outputs correspond to cell_tracking/metric_learning, respectively:\n",
       "\n",
       "```\n",
       "│\n",
       "├── logs                  # Logs generated by Hydra and PyTorch Lightning loggers in the cell tracking model training\n",
       "│   ├── runs                    # Folder for logs generated from single runs of the full model\n",
       "│   │   ├── 2021-02-15              # Date of executing run\n",
       "│   │   │   ├── 16-50-49                # Hour of executing run\n",
       "│   │   │   │   ├── .hydra                  # Hydra logs\n",
       "│   │   │   │   ├── wandb                   # Weights&Biases logs\n",
       "│   │   │   │   ├── checkpoints             # Training checkpoints\n",
       "│   │   │   │   └── ...                     # Any other thing saved during training\n",
       "│   │   │   ├── ...\n",
       "│   │   │   └── ...\n",
       "│   │   ├── ...\n",
       "│   └── └── ...\n",
       "│   │\n",
       "├── outputs                     # Outputs generated by Hydra and tensorboard loggers when training deep metric learning model\n",
       "│   ├── runs                    # Folder for logs generated from single runs\n",
       "│   │   ├── 2021-02-15              # Date of executing run\n",
       "│   │   │   ├── 16-50-49                # Hour of executing run\n",
       "│   │   │   │   ├── .hydra                  # Hydra logs\n",
       "│   │   │   │   ├── logs_<exp_name>         # Any other thing saved during training - included checkpoints and logs\n",
       "│   │   │   │   └── all_params.pth         # A dictionary consisting of all the relevant information (model state dicts and other parameters that are used for feature extraction)\n",
       "│   │   │   ├── ...\n",
       "│   │   │   └── ...\n",
       "│   │   ├── ...\n",
       "│   └── └── ...\n",
       "│\n",
       "```\n",
       "<br><br>\n",
       "</details>\n",
       "\n",
       "<h2>Evaluation code</h2>\n",
       "<details>\n",
       "\n",
       "To run evaluation, we provide an example script (submitted to CTC) and all the relevant files to run our code in ```src/inference``` folder, details below:\n",
       "```\n",
       "SEQUENCE=\"01\"\n",
       "FOV=\"0\"\n",
       "DATASET=\"${PWD}/../Fluo-N2DH-SIM+\" # path/to/dataset/dir\n",
       "CODE_TRA=\"${PWD}\" # path/to/inference_of_tracking/algorithm(ours)\n",
       "MODEL_METRIC_LEARNING=\"${PWD}/parameters/Features_Models/Fluo-N2DH-SIM+/all_params.pth\" # path/to/MODEL_METRIC_LEARNING/parameters\n",
       "MODEL_PYTORCH_LIGHTNING=\"${PWD}/parameters/Tracking_Models/Fluo-N2DH-SIM+/checkpoints/epoch=132.ckpt\" # path/to/tracking_model/parameters\n",
       "CODE_SEG=\"${PWD}/seg_code/\" # path/to/seg/algorithm\n",
       "SEG_MODEL=\"${PWD}/parameters/Seg_Models/Fluo-N2DH-SIM+/\" # path/to/seg_model/parameters\n",
       "MODALITY=\"2D\"  # dataset modality\n",
       "\n",
       "# seg prediction\n",
       "python ${CODE_SEG}/Inference2D.py --gpu_id 0 --model_path ${SEG_MODEL} --sequence_path \"${DATASET}/${SEQUENCE}\" --output_path \"${DATASET}/${SEQUENCE}_SEG_RES\" --edge_dist 3 --edge_thresh=0.3 --min_cell_size 100 --max_cell_size 1000000 --fov 0 --centers_sigmoid_threshold 0.8 --min_center_size 10 --pre_sequence_frames 4 --data_format NCHW --save_intermediate --save_intermediate_path ${DATASET}/${SEQUENCE}_SEG_intermediate\n",
       "\n",
       "# cleanup\n",
       "rm -r \"${DATASET}/${SEQUENCE}_SEG_intermediate\"\n",
       "\n",
       "# Finish segmentation - start tracking\n",
       "\n",
       "# our model needs CSVs, so let's create from image and segmentation.\n",
       "python ${CODE_TRA}/preprocess_seq2graph_clean.py -cs 20 -ii \"${DATASET}/${SEQUENCE}\" -iseg \"${DATASET}/${SEQUENCE}_SEG_RES\" -im \"${MODEL_METRIC_LEARNING}\" -oc \"${DATASET}/${SEQUENCE}_CSV\"\n",
       "\n",
       "# run the prediction\n",
       "python ${CODE_TRA}/inference_clean.py -mp \"${MODEL_PYTORCH_LIGHTNING}\" -ns \"${SEQUENCE}\" -oc \"${DATASET}\"\n",
       "\n",
       "# postprocess\n",
       "python ${CODE_TRA}/postprocess_clean.py -modality \"${MODALITY}\" -iseg \"${DATASET}/${SEQUENCE}_SEG_RES\" -oi \"${DATASET}/${SEQUENCE}_RES_inference\"\n",
       "\n",
       "rm -r \"${DATASET}/${SEQUENCE}_CSV\" \"${DATASET}/${SEQUENCE}_RES_inference\" \"${DATASET}/${SEQUENCE}_SEG_RES\"\n",
       "```\n",
       "You should create the same script as above with the relevant parameters to trained models (which are elaborated above how to produce), In comments, we explain each variable. Please refer to the main paper and read about the segmentation algorithms used.\n",
       "Please refer to read about evaluation-methodology of the challenge here http://celltrackingchallenge.net/evaluation-methodology/ - it is also provided with the Command-line software packages that implement the TRA measure (publicly available in the link)\n",
       "\n",
       "</details>\n",
       "\n",
       "<h2>Google Colab notebook example</h2>\n",
       "<p><a href=\"https://colab.research.google.com/github/talbenha/cell-tracker-gnn/blob/main/notebooks/training_example.ipynb\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a></p>\n",
       "<h2>Pretrained Models</h2>\n",
       "<p>The submitted softwate and pretrained models to the <a href=\"http://celltrackingchallenge.net/\">cell tracking challenge</a> are available at the <a href=\"https://github.com/talbenha/cell-tracker-gnn/releases\">Releases</a></p>\n",
       "<h2>Citation</h2>\n",
       "<p>If you find either the code or the paper useful for your research, cite our paper:\n",
       "<code>sh\n",
       "@inproceedings{ben2022graph,\n",
       "title={Graph Neural Network for Cell Tracking in Microscopy Videos},\n",
       "author={Ben-Haim, Tal and Riklin-Raviv, Tammy},\n",
       "booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},\n",
       "year={2022},\n",
       "}</code></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import markdown\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "with open('.\\cell-tracker-gnn\\README.md', 'r', encoding='utf-8') as file:\n",
    "    readme_content = file.read()\n",
    "\n",
    "\n",
    "html_content = markdown.markdown(readme_content)\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data/CTC/Training/Fluo-N2DH-SIM+  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cấu trúc:** 2 thư mục con 01 và 01_GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* thư mục 01 dùng để train, thư mục 01_GT là ground truth data tượng trưng cho 'label'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* trong thư mục 01_GT có 2 thư mục con là TRA và SEG, TRA là correct infor cho tracking task. SEG là correnct infor cho segmentation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hầu hết seg folder không dùng, duy nhất có bộ Training/PhC-C2DH-U373/01_ST dùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lý do:Chất lượng dữ liệu:\n",
    "<br>Dữ liệu trong thư mục 01_ST SEG có thể đã được kiểm tra và xác nhận là có chất lượng cao hơn, do đó nó được sử dụng cho mục đích tracking.\n",
    "<br>Không sử dụng các thư mục SEG khác:\n",
    "Các thư mục SEG khác có thể không được sử dụng vì chúng có thể không đạt yêu cầu về chất lượng hoặc không phù hợp cho việc tracking trong ngữ cảnh của dự án này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cấu trức thư mục\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Config: chứa các file cấu hình cho Hydra (ko qt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Log:Chứa các log được tạo bởi Hydra và PyTorch Lightning loggers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outputs: output được tạo bởi Hydra và tensorboard loggers khi huấn luyện mô hình deep metric learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. datamodules:Chứa các Lightning datamodules và các tệp dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 datasets: Triển khai dataset cho đồ thị<br>\n",
    "1.2 extract_features: Trích xuất các đặc trưng dùng cho đồ thị.<br>\n",
    "1.3 celltrack_datamodule.py: Lightning datamodule triển khai chia tách cho train, valid và test sử dụng các chuỗi riêng biệt.<br>\n",
    "1.4 celltrack_datamodule_mulSeq.py: Lightning datamodule triển khai chia tách cho train, valid và test sử dụng các chuỗi kết hợp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* chú thích: Chuỗi: Tập hợp các hình ảnh hoặc khung hình liên tiếp theo thời gian.\n",
    "* Chia tách các chuỗi riêng biệt: *Nếu bạn có 3 chuỗi (A, B, C), bạn có thể sử dụng chuỗi A cho huấn luyện, chuỗi B cho kiểm định và chuỗi C cho kiểm tra*\n",
    "* Kết hợp các chuỗi: *Các chuỗi được kết hợp lại và chia thành các tập dữ liệu huấn luyện, kiểm định và kiểm tra.* ( Để tăng cường dữ liệu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Metric (theo dõi hiệu suất)\n",
    "3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. modules: Triển khai các models.\n",
    "* celltrack_model.py: Triển khai mô hình hoàn chỉnh.\n",
    "* edge_mpnn.py: Triển khai message passing theo cạnh.\n",
    "* mlp.py: Triển khai multilayer perceptron.\n",
    "* pdn_conv.py: Triển khai PDN-Conv. (Probabilistic Distance-based Neighborhood Convolution là một loại lớp convolution được thiết kế để làm việc với dữ liệu đồ thị (graph data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. celltrack_plmodel.py: Lightning model triển khai quy trình huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 file .py cuối cùng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. run.py: Chạy quá trình huấn luyện của mô hình hoàn chỉnh với bất kỳ cấu hình pipeline nào từ tệp configs/config.yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. run_feat_extract.py: Chạy pipeline trích xuất đặc trưng theo cấu hình từ tệp configs/feat_extract/feat_extract.yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. run_train_metric_learning.py: Chạy quá trình huấn luyện với bất kỳ cấu hình nào từ các tệp cấu hình trong configs/metric_learning/...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
